#!/usr/bin/env python3
"""
MVP Testing CLI for RC Release Automation

This script allows testing the core functionality independently:
- GitHub PR fetching
- Release notes generation 
- CRQ document generation
- Configuration validation

Usage:
    python test_cli.py --help
    python test_cli.py --test-all
    python test_cli.py --test-prs --prod-version v1.2.3 --new-version v1.3.0
    python test_cli.py --test-docs --service-name example-service --rc-name "John Doe"
"""

import sys
from pathlib import Path

# Add project root to Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import argparse
import json
import os
from typing import Dict, Any
from types import SimpleNamespace

from src.utils.logging import get_logger
from src.config.config import load_config
from src.github.fetch_prs import fetch_prs
from notes.release_notes import render_release_notes, render_release_notes_markdown
from src.crq.generate_crqs import generate_crqs


def validate_confluence_format(content: str) -> bool:
    """Validate that content is proper Confluence wiki markup."""
    required_elements = [
        "h1.",  # Confluence header
        "||",   # Confluence table syntax
        "|",    # Table rows
        "Artifact",  # Key sections
        "Release Date/Time",
        "GraphQL Schema Changes",
        "{panel:",  # Confluence panels
        "Sign-off"  # Release approval workflow
    ]
    
    for element in required_elements:
        if element not in content:
            return False
    
    # Check for proper table structure
    lines = content.split('\n')
    table_lines = [line for line in lines if line.strip().startswith('||') or line.strip().startswith('|')]
    
    return len(table_lines) >= 5  # Should have multiple table rows


def validate_crq_format(content: str, day_type: str) -> bool:
    """Validate that content is proper enterprise CRQ format."""
    required_elements = [
        "Summary:",
        "===== Description Section ======",
        "Application Name:",
        "Namespace:",
        "Region of deployment:",
        "What is the criticality of change",
        "How have we validated this change",
        "What is the blast radius",
        "===== Implementation Plan Section ======",
        "Assembly:",
        "Service name:",
        "Platform:",
        "Artifact Version:",
        "Forward Artifact Version",
        "Rollback Artifact Version",
        "Confluence link:",
        "===== Validation Plan Section ======",
        "Dashboard links:",
        "P0 Dashboard",
        "L1 Dashboard", 
        "Services dashboard",
        "===== Backout Plan Section ======",
        "What are the rollback criteria",
        "What are the rollback steps"
    ]
    
    for element in required_elements:
        if element not in content:
            return False
    
    # Day-specific validation
    if day_type == "day1":
        return "Day 1" in content
    elif day_type == "day2": 
        return "Day 2" in content
    
    return True


def create_sample_params(args) -> Dict[str, Any]:
    """Create sample parameters for testing."""
    return {
        "prod_version": args.prod_version or "v1.2.3",
        "new_version": args.new_version or "v1.3.0", 
        "service_name": args.service_name or "example-service",
        "release_type": args.release_type or "standard",
        "rc_name": args.rc_name or "Test User",
        "rc_manager": args.rc_manager or "Test Manager",
        "day1_date": args.day1_date or "2024-01-15",
        "day2_date": args.day2_date or "2024-01-16",
        "channel": "#release-rc",
        "output_dir": args.output_dir or "test_outputs",
        "config_path": getattr(args, 'config_path', "src/config/settings.test.yaml")
    }


def test_configuration():
    """Test configuration loading and validation."""
    logger = get_logger(__name__)
    logger.info("üîß Testing configuration loading...")
    
    try:
        # Use test config to avoid validation errors
        config = load_config("src/config/settings.test.yaml")
        logger.info("‚úÖ Configuration loaded successfully")
        
        # Test required sections
        sections = ["github", "ai", "organization"]
        for section in sections:
            if hasattr(config, section):
                logger.info(f"‚úÖ {section} section present")
            else:
                logger.warning(f"‚ö†Ô∏è {section} section missing")
        
        # Test environment variables
        env_vars = ["GITHUB_TOKEN"]
        for var in env_vars:
            if os.environ.get(var):
                logger.info(f"‚úÖ {var} environment variable set")
            else:
                logger.warning(f"‚ö†Ô∏è {var} environment variable missing")
                
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Configuration test failed: {e}")
        return False


def test_github_integration(params: Dict[str, Any]):
    """Test GitHub PR fetching."""
    logger = get_logger(__name__)
    logger.info("üêô Testing GitHub integration...")
    
    try:
        github_token = os.environ.get("GITHUB_TOKEN")
        
        # Check if we have a real GitHub token (not a test dummy)
        if not github_token or github_token.startswith("dummy-") or len(github_token) < 20:
            logger.warning("‚ö†Ô∏è GITHUB_TOKEN not set or invalid - using mock data")
            # Create mock PRs for testing
            mock_pr = SimpleNamespace()
            mock_pr.number = 123
            mock_pr.title = "Test PR for MVP validation"
            mock_pr.user = SimpleNamespace()
            mock_pr.user.login = "test-user"
            mock_pr.html_url = "https://github.com/test/repo/pull/123"
            mock_pr.labels = []
            mock_pr.body = "This is a test PR for MVP validation"
            
            prs = [mock_pr]
            logger.info(f"‚úÖ Using mock data: {len(prs)} PRs")
        else:
            # Load config and use real GitHub integration
            try:
                config = load_config()
                prs = fetch_prs(params["prod_version"], params["new_version"], config.github)
                logger.info(f"‚úÖ GitHub integration successful: {len(prs)} PRs fetched")
            except Exception as github_error:
                logger.warning(f"‚ö†Ô∏è GitHub API failed ({github_error}), falling back to mock data")
                # Fallback to mock data
                mock_pr = SimpleNamespace()
                mock_pr.number = 124
                mock_pr.title = "Fallback test PR"
                mock_pr.user = SimpleNamespace()
                mock_pr.user.login = "fallback-user"
                mock_pr.html_url = "https://github.com/test/repo/pull/124"
                mock_pr.labels = []
                mock_pr.body = "Fallback PR for testing"
                
                prs = [mock_pr]
                logger.info(f"‚úÖ Using fallback mock data: {len(prs)} PRs")
            
        return prs
        
    except Exception as e:
        logger.error(f"‚ùå GitHub test failed: {e}")
        return []


def test_release_notes(prs: list, params: Dict[str, Any], output_dir: Path):
    """Test release notes generation."""
    logger = get_logger(__name__)
    logger.info("üìù Testing release notes generation...")
    
    try:
        # Load config using the specified path
        config_path = params.get("config_path", "config/settings.yaml")
        config = load_config(config_path)
        
        # Test Confluence format only (enterprise standard)
        confluence_file = render_release_notes(prs, params, output_dir, config=config)
        logger.info(f"‚úÖ Confluence release notes: {confluence_file}")
        
        # Validate file exists and has content
        if confluence_file.exists() and confluence_file.stat().st_size > 0:
            logger.info(f"‚úÖ {confluence_file.name} generated successfully ({confluence_file.stat().st_size} bytes)")
            
            # Validate Confluence format
            content = confluence_file.read_text()
            if validate_confluence_format(content):
                logger.info("‚úÖ Confluence markup format validation passed")
            else:
                logger.error("‚ùå Confluence markup format validation failed")
                return False
        else:
            logger.error(f"‚ùå {confluence_file.name} is empty or missing")
            return False
                
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Release notes test failed: {e}")
        return False


def test_crq_generation(prs: list, params: Dict[str, Any], output_dir: Path):
    """Test CRQ document generation."""
    logger = get_logger(__name__)
    logger.info("üìã Testing CRQ generation...")
    
    try:
        # Load config using the specified path
        config_path = params.get("config_path", "config/settings.yaml")
        config = load_config(config_path)
        
        crq_files = generate_crqs(prs, params, output_dir, config=config)
        logger.info(f"‚úÖ CRQ generation successful: {len(crq_files)} files")
        
        # Validate both CRQ files with format validation
        expected_files = [
            ("crq_day1.txt", "day1"),
            ("crq_day2.txt", "day2")
        ]
        
        for expected_file, day_type in expected_files:
            file_path = output_dir / expected_file
            if file_path.exists() and file_path.stat().st_size > 0:
                logger.info(f"‚úÖ {expected_file} generated successfully ({file_path.stat().st_size} bytes)")
                
                # Validate CRQ format
                content = file_path.read_text()
                if validate_crq_format(content, day_type):
                    logger.info(f"‚úÖ {expected_file} format validation passed")
                else:
                    logger.error(f"‚ùå {expected_file} format validation failed")
                    return False
            else:
                logger.error(f"‚ùå {expected_file} is empty or missing")
                return False
                
        return True
        
    except Exception as e:
        logger.error(f"‚ùå CRQ generation test failed: {e}")
        return False


def test_ai_integration():
    """Test AI client functionality."""
    logger = get_logger(__name__)
    logger.info("ü§ñ Testing AI integration...")
    
    try:
        if not os.environ.get("OPENAI_API_KEY"):
            logger.warning("‚ö†Ô∏è OPENAI_API_KEY not set - AI features will use fallback")
            return True
            
        from src.utils.ai_client import AIClient
        config = load_config()
        ai_client = AIClient(config.ai)
        
        # Test with simple prompt
        test_prompt = "What is the primary risk of deploying code changes to production?"
        response = ai_client.generate_text(test_prompt)
        
        if response and len(response) > 10:
            logger.info(f"‚úÖ AI integration successful (response: {len(response)} chars)")
            return True
        else:
            logger.error("‚ùå AI response is too short or empty")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå AI integration test failed: {e}")
        return False


def create_comprehensive_mock_prs():
    """Create comprehensive mock PR data for testing release notes with multiple categories."""
    prs = []
    
    # Create 5 Schema PRs
    schema_prs_data = [
        {"number": 101, "title": "Add `newField` to User type", "author": "alice", "labels": ["schema", "breaking"]},
        {"number": 102, "title": "Deprecate `oldField` in Product schema", "author": "bob", "labels": ["schema", "deprecation"]},
        {"number": 103, "title": "Rename mutation `createX` to `addX`", "author": "carol", "labels": ["schema", "api", "breaking"]},
        {"number": 104, "title": "Add `status` enum value to Order", "author": "dave", "labels": ["schema", "enhancement"]},
        {"number": 105, "title": "Remove unused type `LegacyFoo`", "author": "eve", "labels": ["schema", "cleanup"]},
    ]
    
    for pr_data in schema_prs_data:
        pr = SimpleNamespace()
        pr.number = pr_data["number"]
        pr.title = pr_data["title"]
        pr.user = SimpleNamespace()
        pr.user.login = pr_data["author"]
        pr.html_url = f"https://github.com/test/repo/pull/{pr_data['number']}"
        pr.labels = []
        for label_name in pr_data["labels"]:
            label = SimpleNamespace()
            label.name = label_name
            pr.labels.append(label)
        pr.body = f"Schema change: {pr_data['title']}"
        prs.append(pr)
    
    # Create 10 Feature/Bugfix PRs
    feature_bugfix_data = [
        {"number": 201, "title": "Fix cart crash on zero quantity", "author": "alice", "labels": ["bug", "cart"], "type": "bugfix"},
        {"number": 202, "title": "Add express checkout button", "author": "bob", "labels": ["feature", "checkout"], "type": "feature"},
        {"number": 203, "title": "Improve search performance", "author": "carol", "labels": ["enhancement", "performance"], "type": "feature"},
        {"number": 204, "title": "Fix rounding error in totals", "author": "dave", "labels": ["bug", "calculation"], "type": "bugfix"},
        {"number": 205, "title": "Add UI flag for beta users", "author": "eve", "labels": ["feature", "ui"], "type": "feature"},
        {"number": 206, "title": "Remove logging noise in production", "author": "frank", "labels": ["bug", "logging"], "type": "bugfix"},
        {"number": 207, "title": "Add bulk-update mutation", "author": "grace", "labels": ["feature", "api"], "type": "feature"},
        {"number": 208, "title": "Fix memory leak in subscription service", "author": "heidi", "labels": ["bug", "memory"], "type": "bugfix"},
        {"number": 209, "title": "Add pagination to comments query", "author": "ivy", "labels": ["feature", "pagination"], "type": "feature"},
        {"number": 210, "title": "Fix timezone handling on events", "author": "judy", "labels": ["bug", "timezone"], "type": "bugfix"},
    ]
    
    for pr_data in feature_bugfix_data:
        pr = SimpleNamespace()
        pr.number = pr_data["number"]
        pr.title = pr_data["title"]
        pr.user = SimpleNamespace()
        pr.user.login = pr_data["author"]
        pr.html_url = f"https://github.com/test/repo/pull/{pr_data['number']}"
        pr.labels = []
        for label_name in pr_data["labels"]:
            label = SimpleNamespace()
            label.name = label_name
            pr.labels.append(label)
        pr.body = f"{pr_data['type'].title()} change: {pr_data['title']}"
        prs.append(pr)
    
    # Create 5 International PRs
    international_data = [
        {"number": 301, "title": "Add locale 'es-MX' support", "author": "sam", "labels": ["i18n", "locale"]},
        {"number": 302, "title": "Update date formats for UK locale", "author": "tony", "labels": ["i18n", "formatting"]},
        {"number": 303, "title": "Fix RTL layout on checkout page", "author": "uma", "labels": ["i18n", "rtl", "ui"]},
        {"number": 304, "title": "Add currency 'INR' formatting", "author": "victor", "labels": ["i18n", "currency"]},
        {"number": 305, "title": "Remove outdated locale 'fr-CA'", "author": "wendy", "labels": ["i18n", "cleanup"]},
    ]
    
    international_prs = []
    for pr_data in international_data:
        pr = SimpleNamespace()
        pr.number = pr_data["number"]
        pr.title = pr_data["title"]
        pr.user = SimpleNamespace()
        pr.user.login = pr_data["author"]
        pr.html_url = f"https://github.com/test/repo/pull/{pr_data['number']}"
        pr.labels = []
        for label_name in pr_data["labels"]:
            label = SimpleNamespace()
            label.name = label_name
            pr.labels.append(label)
        pr.body = f"International change: {pr_data['title']}"
        international_prs.append(pr)
    
    return prs, international_prs


def test_comprehensive_release_notes(params: Dict[str, Any]):
    """Test comprehensive document generation with realistic mock data."""
    logger = get_logger(__name__)
    logger.info("üîç Testing comprehensive document generation with realistic data...")
    
    try:
        # Create comprehensive mock data
        prs, international_prs = create_comprehensive_mock_prs()
        logger.info(f"Created {len(prs)} main PRs and {len(international_prs)} international PRs")
        
        # Setup output directory
        output_dir = Path(params["output_dir"])
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Update params to include international PRs
        params["international_prs"] = international_prs
        
        # Test 1: Release notes generation
        logger.info("üìù Testing release notes generation...")
        release_notes_success = test_release_notes(prs, params, output_dir)
        
        # Test 2: CRQ generation  
        logger.info("üìã Testing CRQ generation...")
        crq_success = test_crq_generation(prs, params, output_dir)
        
        # Overall validation
        if release_notes_success and crq_success:
            logger.info("‚úÖ Comprehensive document generation test passed!")
            
            # Validate all expected files are present
            expected_files = ["release_notes.txt", "crq_day1.txt", "crq_day2.txt"]
            all_files_present = True
            
            for expected_file in expected_files:
                file_path = output_dir / expected_file
                if file_path.exists():
                    logger.info(f"‚úÖ {expected_file}: {file_path.stat().st_size:,} bytes")
                else:
                    logger.error(f"‚ùå Missing: {expected_file}")
                    all_files_present = False
            
            if all_files_present:
                # Show content analysis for release notes
                release_notes_file = output_dir / "release_notes.txt"
                content = release_notes_file.read_text()
                schema_count = content.count("schema")
                feature_count = content.count("feature")
                international_count = content.count("i18n") + content.count("locale")
                
                logger.info(f"üìä Release Notes Content Analysis:")
                logger.info(f"  - Schema references: {schema_count}")
                logger.info(f"  - Feature references: {feature_count}")
                logger.info(f"  - International references: {international_count}")
                logger.info(f"  - Total size: {release_notes_file.stat().st_size:,} bytes")
                
                return True
            else:
                return False
        else:
            logger.error("‚ùå Document generation failed")
            return False
        
    except Exception as e:
        logger.error(f"‚ùå Comprehensive document generation test failed: {e}")
        return False


def run_all_tests(params: Dict[str, Any]):
    """Run comprehensive test suite."""
    logger = get_logger(__name__)
    logger.info("üöÄ Running comprehensive MVP test suite...")
    
    # Setup test environment
    output_dir = Path(params["output_dir"])
    output_dir.mkdir(parents=True, exist_ok=True)
    
    test_results = {}
    
    # Test 1: Configuration
    test_results["config"] = test_configuration()
    
    # Test 2: AI Integration
    test_results["ai"] = test_ai_integration()
    
    # Test 3: GitHub Integration
    prs = test_github_integration(params)
    test_results["github"] = len(prs) > 0
    
    # Test 4: Release Notes (basic)
    test_results["release_notes"] = test_release_notes(prs, params, output_dir)
    
    # Test 5: CRQ Generation
    test_results["crq"] = test_crq_generation(prs, params, output_dir)
    
    # Test 6: Comprehensive Release Notes (NEW)
    test_results["comprehensive"] = test_comprehensive_release_notes(params)
    
    # Summary
    logger.info("\nüéØ Test Results Summary:")
    passed = 0
    total = len(test_results)
    
    for test_name, result in test_results.items():
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        logger.info(f"  {test_name:<15}: {status}")
        if result:
            passed += 1
    
    logger.info(f"\nüìä Overall: {passed}/{total} tests passed")
    
    if passed == total:
        logger.info("üéâ All tests passed! MVP is ready for deployment.")
    else:
        logger.error("‚ö†Ô∏è Some tests failed. Please check the issues above.")
        
    # Show generated files
    logger.info(f"\nüìÅ Generated files in {output_dir}:")
    for file_path in sorted(output_dir.glob("*")):
        size = file_path.stat().st_size
        logger.info(f"  - {file_path.name} ({size:,} bytes)")
    
    return passed == total


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="MVP Testing CLI for RC Release Automation",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    # Test selection
    parser.add_argument("--test-all", action="store_true",
                       help="Run comprehensive test suite")
    parser.add_argument("--test-config", action="store_true",
                       help="Test configuration loading only")
    parser.add_argument("--test-prs", action="store_true", 
                       help="Test GitHub PR fetching only")
    parser.add_argument("--test-docs", action="store_true",
                       help="Test document generation only")
    parser.add_argument("--test-ai", action="store_true",
                       help="Test AI integration only")
    parser.add_argument("--test-comprehensive", action="store_true",
                       help="Test comprehensive release notes with 5 schema + 10 feature + 5 international PRs")
    
    # Release parameters (optional - will use defaults for testing)
    parser.add_argument("--prod-version", 
                       help="Production version (default: v1.2.3)")
    parser.add_argument("--new-version",
                       help="New version (default: v1.3.0)")
    parser.add_argument("--service-name",
                       help="Service name (default: example-service)")
    parser.add_argument("--release-type", choices=["standard", "hotfix", "ebf"],
                       help="Release type (default: standard)")
    parser.add_argument("--rc-name",
                       help="RC name (default: Test RC)")
    parser.add_argument("--rc-manager", 
                       help="RC manager (default: Test Manager)")
    parser.add_argument("--day1-date",
                       help="Day 1 date (default: 2024-01-15)")
    parser.add_argument("--day2-date",
                       help="Day 2 date (default: 2024-01-16)")
    parser.add_argument("--output-dir",
                       default="test_outputs",
                       help="Output directory for test results (default: test_outputs)")
    parser.add_argument("--config-path",
                       default="config/settings.yaml",
                       help="Path to configuration file (default: config/settings.yaml)")
                       
    return parser.parse_args()


def main():
    """Main testing function."""
    logger = get_logger(__name__)
    
    try:
        args = parse_args()
        params = create_sample_params(args)
        
        # Show test parameters
        logger.info("üß™ RC Release Automation - MVP Testing")
        logger.info("=" * 50)
        logger.info(f"Service: {params['service_name']} {params['prod_version']} ‚Üí {params['new_version']}")
        logger.info(f"RC: {params['rc_name']} | Manager: {params['rc_manager']}")
        logger.info(f"Dates: {params['day1_date']} (Day 1) | {params['day2_date']} (Day 2)")
        logger.info("=" * 50)
        
        # Setup output directory
        output_dir = Path(params["output_dir"])
        output_dir.mkdir(parents=True, exist_ok=True)
        
        success = True
        
        # Run selected tests
        if args.test_all:
            success = run_all_tests(params)
        elif args.test_config:
            success = test_configuration()
        elif args.test_ai:
            success = test_ai_integration()
        elif args.test_prs:
            prs = test_github_integration(params)
            success = len(prs) > 0
        elif args.test_docs:
            # Need PRs for doc generation
            prs = test_github_integration(params)
            if prs:
                success = (test_release_notes(prs, params, output_dir) and 
                          test_crq_generation(prs, params, output_dir))
        elif args.test_comprehensive:
            success = test_comprehensive_release_notes(params)
        else:
            # Default: run all tests
            success = run_all_tests(params)
            
        if success:
            logger.info("\nüéâ Testing completed successfully!")
            exit(0)
        else:
            logger.error("\n‚ùå Testing failed!")
            exit(1)
            
    except KeyboardInterrupt:
        logger.info("\nTesting interrupted by user")
        exit(130)
    except Exception as e:
        logger.error(f"Testing failed with error: {e}")
        import traceback
        logger.error(f"Full traceback: {traceback.format_exc()}")
        exit(1)


if __name__ == "__main__":
    main() 